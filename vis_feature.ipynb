{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_encoder.lseg_encoder.feature_extractor import LSeg_FeatureExtractor\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **\n",
      "[Debug--LSeg_FeatureExtractor]LSegModule(\n",
      "  (net): LSegNet(\n",
      "    (clip_pretrained): CLIP(\n",
      "      (visual): VisionTransformer(\n",
      "        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer): Transformer(\n",
      "          (resblocks): Sequential(\n",
      "            (0): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): Sequential(\n",
      "          (0): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (11): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pretrained): Module(\n",
      "      (model): VisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "        (patch_drop): Identity()\n",
      "        (norm_pre): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (2): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (3): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (4): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (5): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (6): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (7): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (8): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (9): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (10): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (11): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (12): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (13): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (14): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (15): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (16): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (17): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (18): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (19): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (20): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (21): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (22): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (23): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (fc_norm): Identity()\n",
      "        (head_drop): Dropout(p=0.0, inplace=False)\n",
      "        (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "      )\n",
      "      (act_postprocess1): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (act_postprocess2): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (act_postprocess3): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (act_postprocess4): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (scratch): Module(\n",
      "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (refinenet1): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet2): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet3): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet4): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output_conv): Sequential(\n",
      "        (0): Interpolate()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[Debug--LSeg_FeatureExtractor]scales: [0.75, 1.0, 1.25, 1.75]\n",
      "[Debug--LSeg_FeatureExtractor]img_size: [360, 480]\n",
      "MultiEvalModule: base_size 520, crop_size 480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSeg_FeatureExtractor(\n",
       "  (evaluator): MultiEvalModule(\n",
       "    (module): LSegModule(\n",
       "      (net): LSegNet(\n",
       "        (clip_pretrained): CLIP(\n",
       "          (visual): VisionTransformer(\n",
       "            (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "            (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (transformer): Transformer(\n",
       "              (resblocks): Sequential(\n",
       "                (0): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (1): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (2): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (3): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (4): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (5): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (6): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (7): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (8): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (9): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (10): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (11): ResidualAttentionBlock(\n",
       "                  (attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): Sequential(\n",
       "                    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (gelu): QuickGELU()\n",
       "                    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (transformer): Transformer(\n",
       "            (resblocks): Sequential(\n",
       "              (0): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (1): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (2): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (3): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (4): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (5): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (6): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (7): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (8): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (9): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (10): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (11): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (token_embedding): Embedding(49408, 512)\n",
       "          (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pretrained): Module(\n",
       "          (model): VisionTransformer(\n",
       "            (patch_embed): PatchEmbed(\n",
       "              (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "              (norm): Identity()\n",
       "            )\n",
       "            (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "            (patch_drop): Identity()\n",
       "            (norm_pre): Identity()\n",
       "            (blocks): Sequential(\n",
       "              (0): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (1): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (2): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (3): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (4): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (5): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (6): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (7): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (8): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (9): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (10): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (11): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (12): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (13): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (14): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (15): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (16): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (17): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (18): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (19): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (20): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (21): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (22): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (23): Block(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (q_norm): Identity()\n",
       "                  (k_norm): Identity()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (fc_norm): Identity()\n",
       "            (head_drop): Dropout(p=0.0, inplace=False)\n",
       "            (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "          )\n",
       "          (act_postprocess1): Sequential(\n",
       "            (0): ProjectReadout(\n",
       "              (project): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "            (1): Transpose()\n",
       "            (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "            (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (act_postprocess2): Sequential(\n",
       "            (0): ProjectReadout(\n",
       "              (project): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "            (1): Transpose()\n",
       "            (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "            (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          )\n",
       "          (act_postprocess3): Sequential(\n",
       "            (0): ProjectReadout(\n",
       "              (project): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "            (1): Transpose()\n",
       "            (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act_postprocess4): Sequential(\n",
       "            (0): ProjectReadout(\n",
       "              (project): Sequential(\n",
       "                (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "            (1): Transpose()\n",
       "            (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (scratch): Module(\n",
       "          (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (refinenet1): FeatureFusionBlock_custom(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet2): FeatureFusionBlock_custom(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet3): FeatureFusionBlock_custom(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet4): FeatureFusionBlock_custom(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit_custom(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (output_conv): Sequential(\n",
       "            (0): Interpolate()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = LSeg_FeatureExtractor(debug=True)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1371688/1952573118.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  feature_map_render = torch.load('feature_map.pt')\n"
     ]
    }
   ],
   "source": [
    "feature_map_render = torch.load('feature_map.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 360, 480])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map_render.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug--LSeg_FeatureExtractor]image_features: torch.Size([172800, 512]), text_features: torch.Size([150, 512])\n"
     ]
    }
   ],
   "source": [
    "image = feature_extractor.features_to_image(feature_map_render.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 480, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_image = Image.fromarray(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=480x360>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonoGS_Semantic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
