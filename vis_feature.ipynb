{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "import clip\n",
    "import encoding.utils as utils\n",
    "from encoding.models.sseg import BaseNet\n",
    "\n",
    "from feature_encoder.lseg_encoder.modules.lseg_module import LSegModule\n",
    "from feature_encoder.lseg_encoder.additional_utils.encoding_models import MultiEvalModule as LSeg_MultiEvalModule\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSeg_args = namedtuple('LSeg_args', ['weights', 'data_path', 'dataset', 'backbone', \n",
    "                                     'aux', 'ignore_index', 'scale_inv', 'widehead',\n",
    "                                     'widehead_hr', 'img_size', 'labels_path'])\n",
    "adepallete = [0,0,0,120,120,120,180,120,120,6,230,230,80,50,50,4,200,3,120,120,80,140,140,140,204,5,255,230,230,230,4,250,7,224,5,255,235,255,7,150,5,61,120,120,70,8,255,51,255,6,82,143,255,140,204,255,4,255,51,7,204,70,3,0,102,200,61,230,250,255,6,51,11,102,255,255,7,71,255,9,224,9,7,230,220,220,220,255,9,92,112,9,255,8,255,214,7,255,224,255,184,6,10,255,71,255,41,10,7,255,255,224,255,8,102,8,255,255,61,6,255,194,7,255,122,8,0,255,20,255,8,41,255,5,153,6,51,255,235,12,255,160,150,20,0,163,255,140,140,140,250,10,15,20,255,0,31,255,0,255,31,0,255,224,0,153,255,0,0,0,255,255,71,0,0,235,255,0,173,255,31,0,255,11,200,200,255,82,0,0,255,245,0,61,255,0,255,112,0,255,133,255,0,0,255,163,0,255,102,0,194,255,0,0,143,255,51,255,0,0,82,255,0,255,41,0,255,173,10,0,255,173,255,0,0,255,153,255,92,0,255,0,255,255,0,245,255,0,102,255,173,0,255,0,20,255,184,184,0,31,255,0,255,61,0,71,255,255,0,204,0,255,194,0,255,82,0,10,255,0,112,255,51,0,255,0,194,255,0,122,255,0,255,163,255,153,0,0,255,10,255,112,0,143,255,0,82,0,255,163,255,0,255,235,0,8,184,170,133,0,255,0,255,92,184,0,255,255,0,31,0,184,255,0,214,255,255,0,112,92,255,0,0,224,255,112,224,255,70,184,160,163,0,255,153,0,255,71,255,0,255,0,163,255,204,0,255,0,143,0,255,235,133,255,0,255,0,235,245,0,255,255,0,122,255,245,0,10,190,212,214,255,0,0,204,255,20,0,255,255,255,0,0,153,255,0,41,255,0,255,204,41,0,255,41,255,0,173,0,255,0,245,255,71,0,255,122,0,255,0,255,184,0,92,255,184,255,0,0,133,255,255,214,0,25,194,194,102,255,0,92,0,255]\n",
    "\n",
    "def get_labels(dataset, root_path=None):\n",
    "    labels = []\n",
    "    # NOTE：\n",
    "    if root_path is None:\n",
    "        path = 'label_files/{}_objectInfo150.txt'.format(dataset)\n",
    "    else:\n",
    "        path = root_path + '/label_files/{}_objectInfo150.txt'.format(dataset)\n",
    "    assert os.path.exists(path), '*** Error : {} not exist !!!'.format(path)\n",
    "    f = open(path, 'r') \n",
    "    lines = f.readlines()      \n",
    "    for line in lines: \n",
    "        label = line.strip().split(',')[-1].split(';')[0]\n",
    "        labels.append(label)\n",
    "    f.close()\n",
    "    if dataset in ['ade20k']:\n",
    "        labels = labels[1:]\n",
    "    return labels\n",
    "\n",
    "def get_legend_patch(npimg, new_palette, labels):\n",
    "    out_img = Image.fromarray(npimg.squeeze().astype('uint8'))\n",
    "    out_img.putpalette(new_palette)\n",
    "    u_index = np.unique(npimg)\n",
    "    patches = []\n",
    "    for i, index in enumerate(u_index):\n",
    "        label = labels[index]\n",
    "        cur_color = [new_palette[index * 3] / 255.0, new_palette[index * 3 + 1] / 255.0, new_palette[index * 3 + 2] / 255.0]\n",
    "        red_patch = mpatches.Patch(color=cur_color, label=label)\n",
    "        patches.append(red_patch)\n",
    "    return out_img, patches\n",
    "\n",
    "class LSeg_FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(LSeg_FeatureExtractor, self).__init__()\n",
    "        self.debug = debug\n",
    "        args = LSeg_args(weights='/home/MonoGS_Semantic/checkpoints/demo_e200.ckpt', \n",
    "                        data_path=None, \n",
    "                        dataset='ignore', \n",
    "                        backbone='clip_vitl16_384',\n",
    "                        aux=False,\n",
    "                        ignore_index = 255,\n",
    "                        scale_inv=False,\n",
    "                        widehead=True,\n",
    "                        widehead_hr=False,\n",
    "                        img_size=[480, 360],\n",
    "                        labels_path='/home/MonoGS_Semantic/feature_encoder/lseg_encoder')\n",
    "        \n",
    "        module = LSegModule.load_from_checkpoint(\n",
    "            checkpoint_path=args.weights,\n",
    "            data_path=args.data_path,\n",
    "            dataset=args.dataset,\n",
    "            backbone=args.backbone,\n",
    "            aux=args.aux,\n",
    "            num_features=256,\n",
    "            aux_weight=0,\n",
    "            se_loss=False,\n",
    "            se_weight=0,\n",
    "            base_lr=0,\n",
    "            batch_size=1,\n",
    "            max_epochs=0,\n",
    "            ignore_index=args.ignore_index,\n",
    "            dropout=0.0,\n",
    "            scale_inv=args.scale_inv,\n",
    "            augment=False,\n",
    "            no_batchnorm=False,\n",
    "            widehead=args.widehead,\n",
    "            widehead_hr=args.widehead_hr,\n",
    "            map_locatin=\"cpu\",\n",
    "            arch_option=0,\n",
    "            block_depth=0,\n",
    "            activation='lrelu',\n",
    "            labels_path=args.labels_path\n",
    "        )\n",
    "        self.labels = get_labels('ade20k', root_path=args.labels_path)\n",
    "        self.input_transform = module.val_transform\n",
    "        self.num_classes = len(self.labels)\n",
    "        \n",
    "        if isinstance(module.net, BaseNet):\n",
    "            model = module.net\n",
    "        else:\n",
    "            model = module\n",
    "            \n",
    "        model = model.eval()\n",
    "        model = model.cpu()\n",
    "        model = model.half()\n",
    "        # self._log(model)\n",
    "        \n",
    "        self.scales = [0.75, 1.0, 1.25, 1.75]\n",
    "        self.img_size = args.img_size\n",
    "        self._log(f\"scales: {self.scales}\")\n",
    "        self._log(f\"img_size: {self.img_size}\")\n",
    "        \n",
    "        self.evaluator = LSeg_MultiEvalModule(model, self.num_classes, scales=self.scales, flip=True).cuda()\n",
    "        self.evaluator.eval()\n",
    "        self.evaluator.half()\n",
    "    \n",
    "    def _log(self, text):\n",
    "        if self.debug:\n",
    "            print(f\"[Debug--LSeg_FeatureExtractor]{text}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def preprocess(self, image):\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "            image = self.input_transform(image).unsqueeze(0)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.input_transform(image).unsqueeze(0)\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            if image.dim() == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "            elif image.dim() == 4:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported input shape. Supported shapes: (C, H, W), (1, C, H, W)\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Supported types: str (file path), numpy.ndarray, torch.Tensor\")\n",
    "        # self._log(f\"input size: {image.shape}\")\n",
    "        image = F.interpolate(image, size=(self.img_size[0], self.img_size[1]),\n",
    "                                      mode=\"bilinear\", align_corners=True)\n",
    "        # self._log(f\"resize size: {image.shape}\")\n",
    "        return image.half()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward_feature(self, image: torch.Tensor) -> np.ndarray:\n",
    "        with autocast(\"cuda\"):\n",
    "            output_features = self.evaluator.parallel_forward(image, return_feature=True)\n",
    "            # self._log(f\"resize output_features: {output_features[0].shape}\")\n",
    "            return output_features[0] # .cpu().numpy().astype(np.float16)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, image):\n",
    "        with autocast(\"cuda\"):\n",
    "            time_start = time.time()\n",
    "            image_tensor = self.preprocess(image)\n",
    "            feature = self.forward_feature(image_tensor)\n",
    "            numpy_feature = feature.cpu().numpy().astype(np.float16)\n",
    "            rgb_render, patches = self.features_to_image(feature) # free memory\n",
    "            self._log(f\"output_features: {numpy_feature.shape} forward time: {time.time() - time_start}\")\n",
    "            return numpy_feature, rgb_render, patches\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def features_to_image(self, image_features: torch.Tensor, labels_set = None):\n",
    "        with autocast(\"cuda\"):\n",
    "            if len(image_features.shape) == 3:\n",
    "                image_features = image_features.unsqueeze(0)\n",
    "            elif len(image_features.shape) == 4:\n",
    "                pass \n",
    "            else:\n",
    "                raise ValueError(\"Unsupported input shape. Supported shapes: (C, H, W), (1, C, H, W)\")\n",
    "            imshape = image_features.shape\n",
    "            feature_dim = imshape[1]\n",
    "            image_features = image_features.half()\n",
    "            image_features = image_features.permute(0,2,3,1).reshape(-1, feature_dim)\n",
    "            if labels_set is None or len(labels_set) == 0:\n",
    "                text_features = self.evaluator.module.net.clip_pretrained.encode_text(self.evaluator.module.net.text.cuda())\n",
    "                labels_set = self.labels\n",
    "            else:\n",
    "                text = clip.tokenize(labels_set)  \n",
    "                text_features = self.evaluator.module.net.clip_pretrained.encode_text(text.cuda())\n",
    "            # self._log(f\"image_features: {image_features.shape}, text_features: {text_features.shape}\")\n",
    "            # normalized features\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True) \n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True) \n",
    "            \n",
    "            logits_per_image = self.evaluator.module.net.logit_scale * image_features @ text_features.t() \n",
    "            out = logits_per_image.float().view(imshape[0], imshape[2], imshape[3], -1).permute(0,3,1,2) \n",
    "            predicts = torch.max(out, 1)[1].cpu().numpy()\n",
    "            rgb_render, patches = get_legend_patch(predicts, adepallete, labels_set)\n",
    "            rgb_render = rgb_render.convert(\"RGBA\")\n",
    "            del image_features, text_features, logits_per_image, out\n",
    "            torch.cuda.empty_cache()\n",
    "            return np.array(rgb_render), patches\n",
    "    \n",
    "    def draw_patches(self, patches, save_path):\n",
    "        plt.figure()\n",
    "        plt.axis('off')\n",
    "        plt.legend(handles=patches, prop={'size': 8}, ncol=4)\n",
    "        plt.savefig(save_path, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.clf() # Clear the current figure\n",
    "        plt.close() # Close the current figure\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def vis_feature(self, image, outname='test', outdir='vis'):\n",
    "        with autocast(\"cuda\"):\n",
    "            image_tensor = self.preprocess(image)\n",
    "            outputs = self.evaluator.parallel_forward(image_tensor)[0]\n",
    "            predicts = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "            \n",
    "            # save mask\n",
    "            masks = utils.get_mask_pallete(predicts, 'detail')\n",
    "            masks.save(os.path.join(outdir, outname+'.png'))\n",
    "            \n",
    "            # save vis\n",
    "            masks_tensor = torch.tensor(np.array(masks.convert(\"RGB\"), \"f\")) / 255.0\n",
    "            vis_img = (image_tensor[0] + 1) / 2.\n",
    "            vis_img = vis_img.permute(1, 2, 0)  # ->hwc\n",
    "            vis1 = vis_img\n",
    "            vis2 = vis_img * 0.4 + masks_tensor * 0.6\n",
    "            vis3 = masks_tensor\n",
    "            vis = torch.cat([vis1, vis2, vis3], dim=1)\n",
    "            Image.fromarray((vis.cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(outdir, outname+\"_vis.png\"))\n",
    "\n",
    "            # save label vis\n",
    "            seg, patches = get_legend_patch(predicts, adepallete, self.labels)\n",
    "            seg = seg.convert(\"RGBA\")\n",
    "            plt.figure()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(seg)\n",
    "            plt.legend(handles=patches, prop={'size': 8}, ncol=4)\n",
    "            plt.savefig(os.path.join(outdir, outname+\"_legend.png\"), format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.clf()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **\n",
      "[Debug--LSeg_FeatureExtractor]scales: [0.75, 1.0, 1.25, 1.75]\n",
      "[Debug--LSeg_FeatureExtractor]img_size: [480, 360]\n",
      "MultiEvalModule: base_size 520, crop_size 480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lseg = LSeg_FeatureExtractor(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6313, 0.6260, 0.6313,  ..., 0.2942, 0.2942, 0.2864],\n",
       "          [0.6328, 0.6333, 0.6343,  ..., 0.2942, 0.2957, 0.2817],\n",
       "          [0.6235, 0.6411, 0.6401,  ..., 0.2942, 0.2942, 0.2864],\n",
       "          ...,\n",
       "          [0.4182, 0.4121, 0.4021,  ..., 0.7456, 0.7651, 0.7427],\n",
       "          [0.4119, 0.4082, 0.3933,  ..., 0.7349, 0.7437, 0.7446],\n",
       "          [0.4119, 0.4038, 0.4014,  ..., 0.7070, 0.7021, 0.7334]],\n",
       "\n",
       "         [[0.3491, 0.3518, 0.3491,  ..., 0.2079, 0.2079, 0.2000],\n",
       "          [0.3601, 0.3560, 0.3521,  ..., 0.2079, 0.2094, 0.1954],\n",
       "          [0.3647, 0.3586, 0.3577,  ..., 0.2079, 0.2079, 0.2000],\n",
       "          ...,\n",
       "          [0.2379, 0.2318, 0.2218,  ..., 0.3923, 0.4175, 0.3896],\n",
       "          [0.2313, 0.2279, 0.2129,  ..., 0.3865, 0.3972, 0.3916],\n",
       "          [0.2313, 0.2235, 0.2211,  ..., 0.3618, 0.3569, 0.3804]],\n",
       "\n",
       "         [[0.2549, 0.2495, 0.2549,  ..., 0.2705, 0.2549, 0.2471],\n",
       "          [0.2561, 0.2505, 0.2581,  ..., 0.2642, 0.2563, 0.2490],\n",
       "          [0.2471, 0.2487, 0.2634,  ..., 0.2549, 0.2549, 0.2627],\n",
       "          ...,\n",
       "          [0.1124, 0.1166, 0.0963,  ..., 0.0865, 0.1116, 0.0836],\n",
       "          [0.1059, 0.1127, 0.0965,  ..., 0.0807, 0.0945, 0.0765],\n",
       "          [0.1059, 0.1084, 0.1112,  ..., 0.0560, 0.0563, 0.0588]]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_path = \"/home/data/datasets/replica/room0/results/frame000001.jpg\"\n",
    "lseg.preprocess(test_image_path)\n",
    "lseg.vis_feature(test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. feature decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "import clip\n",
    "\n",
    "adepallete = [0,0,0,120,120,120,180,120,120,6,230,230,80,50,50,4,200,3,120,120,80,140,140,140,204,5,255,230,230,230,4,250,7,224,5,255,235,255,7,150,5,61,120,120,70,8,255,51,255,6,82,143,255,140,204,255,4,255,51,7,204,70,3,0,102,200,61,230,250,255,6,51,11,102,255,255,7,71,255,9,224,9,7,230,220,220,220,255,9,92,112,9,255,8,255,214,7,255,224,255,184,6,10,255,71,255,41,10,7,255,255,224,255,8,102,8,255,255,61,6,255,194,7,255,122,8,0,255,20,255,8,41,255,5,153,6,51,255,235,12,255,160,150,20,0,163,255,140,140,140,250,10,15,20,255,0,31,255,0,255,31,0,255,224,0,153,255,0,0,0,255,255,71,0,0,235,255,0,173,255,31,0,255,11,200,200,255,82,0,0,255,245,0,61,255,0,255,112,0,255,133,255,0,0,255,163,0,255,102,0,194,255,0,0,143,255,51,255,0,0,82,255,0,255,41,0,255,173,10,0,255,173,255,0,0,255,153,255,92,0,255,0,255,255,0,245,255,0,102,255,173,0,255,0,20,255,184,184,0,31,255,0,255,61,0,71,255,255,0,204,0,255,194,0,255,82,0,10,255,0,112,255,51,0,255,0,194,255,0,122,255,0,255,163,255,153,0,0,255,10,255,112,0,143,255,0,82,0,255,163,255,0,255,235,0,8,184,170,133,0,255,0,255,92,184,0,255,255,0,31,0,184,255,0,214,255,255,0,112,92,255,0,0,224,255,112,224,255,70,184,160,163,0,255,153,0,255,71,255,0,255,0,163,255,204,0,255,0,143,0,255,235,133,255,0,255,0,235,245,0,255,255,0,122,255,245,0,10,190,212,214,255,0,0,204,255,20,0,255,255,255,0,0,153,255,0,41,255,0,255,204,41,0,255,41,255,0,173,0,255,0,245,255,71,0,255,122,0,255,0,255,184,0,92,255,184,255,0,0,133,255,255,214,0,25,194,194,102,255,0,92,0,255]\n",
    "\n",
    "\n",
    "def get_labels(dataset, root_path=None):\n",
    "    labels = []\n",
    "    # NOTE：\n",
    "    if root_path is None:\n",
    "        path = 'label_files/{}_objectInfo150.txt'.format(dataset)\n",
    "    else:\n",
    "        path = root_path + '/label_files/{}_objectInfo150.txt'.format(dataset)\n",
    "    assert os.path.exists(path), '*** Error : {} not exist !!!'.format(path)\n",
    "    f = open(path, 'r') \n",
    "    lines = f.readlines()      \n",
    "    for line in lines: \n",
    "        label = line.strip().split(',')[-1].split(';')[0]\n",
    "        labels.append(label)\n",
    "    f.close()\n",
    "    if dataset in ['ade20k']:\n",
    "        labels = labels[1:]\n",
    "    return labels\n",
    "\n",
    "def get_legend_patch(npimg, new_palette, labels):\n",
    "    out_img = Image.fromarray(npimg.squeeze().astype('uint8'))\n",
    "    out_img.putpalette(new_palette)\n",
    "    u_index = np.unique(npimg)\n",
    "    patches = []\n",
    "    for i, index in enumerate(u_index):\n",
    "        label = labels[index]\n",
    "        cur_color = [new_palette[index * 3] / 255.0, new_palette[index * 3 + 1] / 255.0, new_palette[index * 3 + 2] / 255.0]\n",
    "        red_patch = mpatches.Patch(color=cur_color, label=label)\n",
    "        patches.append(red_patch)\n",
    "    return out_img, patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSeg_FeatureDecoder(torch.nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(LSeg_FeatureDecoder, self).__init__()\n",
    "        self.debug = debug\n",
    "        args_labels_path = 'feature_encoder/lseg_encoder'\n",
    "        self.clip_pretrained, _ = clip.load(\"ViT-B/32\", device='cuda', jit=False, download_root=\"/tmp/\")\n",
    "        self.clip_pretrained.eval()\n",
    "        self.labels = get_labels('ade20k', root_path=args_labels_path)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)).exp()\n",
    "        self.logit_scale.to('cuda')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def features_to_image(self, image_features: torch.Tensor, labels_set = None):\n",
    "        with autocast(\"cuda\"):\n",
    "            if len(image_features.shape) == 3:\n",
    "                image_features = image_features.unsqueeze(0)\n",
    "            elif len(image_features.shape) == 4:\n",
    "                pass \n",
    "            else:\n",
    "                raise ValueError(\"Unsupported input shape. Supported shapes: (C, H, W), (1, C, H, W)\")\n",
    "            \n",
    "            imshape = image_features.shape\n",
    "            feature_dim = imshape[1]\n",
    "            image_features = image_features.half()\n",
    "            image_features = image_features.permute(0,2,3,1).reshape(-1, feature_dim)\n",
    "            if labels_set is None or len(labels_set) == 0:\n",
    "                labels_set = self.labels\n",
    "            text = clip.tokenize(labels_set)  \n",
    "            text_features = self.clip_pretrained.encode_text(text.cuda())\n",
    "            # normalized features\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True) \n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True) \n",
    "            \n",
    "            logits_per_image = self.logit_scale * image_features @ text_features.t() \n",
    "            out = logits_per_image.float().view(imshape[0], imshape[2], imshape[3], -1).permute(0,3,1,2) \n",
    "            predicts = torch.max(out, 1)[1].cpu().numpy()\n",
    "            rgb_render, patches = get_legend_patch(predicts, adepallete, labels_set)\n",
    "            rgb_render = rgb_render.convert(\"RGBA\")\n",
    "            del image_features, text_features, logits_per_image, out\n",
    "            torch.cuda.empty_cache()\n",
    "        return np.array(rgb_render), patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 360, 480])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240564/711199401.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  feature_map_render = torch.load('feature_map.pt')\n"
     ]
    }
   ],
   "source": [
    "decoder = LSeg_FeatureDecoder(debug=True)\n",
    "feature_map_render = torch.load('feature_map.pt')\n",
    "print(feature_map_render.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder time: 0.09133410453796387\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=480x360>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "image, _ = decoder.features_to_image(feature_map_render)\n",
    "print(f\"decoder time: {time.time() - start_time}\")\n",
    "Image.fromarray(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonoGS_Semantic_feat_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
