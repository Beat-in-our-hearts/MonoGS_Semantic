{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import encoding.utils as utils\n",
    "from encoding.models.sseg import BaseNet\n",
    "from modules.lseg_module import LSegModule\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from additional_utils.encoding_models import MultiEvalModule as LSeg_MultiEvalModule\n",
    "from collections import namedtuple\n",
    "\n",
    "# LSeg_args = namedtuple('LSeg_args', ['model', 'backbone', 'dataset', 'workers', 'base_size',\n",
    "#                                      'crop_size', 'train_split', 'aux', 'se_loss', 'se_weight',\n",
    "#                                      'batch_size', 'test_batch_size', 'no_cuda', 'seed', 'weights',\n",
    "#                                      'eval', 'export', 'acc_bn', 'test_val', 'no_val', 'module',\n",
    "#                                      'data_path', 'no_scaleinv', 'widehead', 'widehead_hr', 'ignore_index',\n",
    "#                                      'label_src', 'jobname', 'no_strict', 'arch_option', 'block_depth',\n",
    "#                                      'activation', 'outdir', 'test_rgb_dir', 'resize_max'])\n",
    "\n",
    "\n",
    "LSeg_args = namedtuple('LSeg_args', ['weights', 'data_path', 'dataset', 'backbone', \n",
    "                                     'aux', 'ignore_index', 'scale_inv', 'widehead',\n",
    "                                     'widehead_hr', 'img_size'])\n",
    "\n",
    "adepallete = [0,0,0,120,120,120,180,120,120,6,230,230,80,50,50,4,200,3,120,120,80,140,140,140,204,5,255,230,230,230,4,250,7,224,5,255,235,255,7,150,5,61,120,120,70,8,255,51,255,6,82,143,255,140,204,255,4,255,51,7,204,70,3,0,102,200,61,230,250,255,6,51,11,102,255,255,7,71,255,9,224,9,7,230,220,220,220,255,9,92,112,9,255,8,255,214,7,255,224,255,184,6,10,255,71,255,41,10,7,255,255,224,255,8,102,8,255,255,61,6,255,194,7,255,122,8,0,255,20,255,8,41,255,5,153,6,51,255,235,12,255,160,150,20,0,163,255,140,140,140,250,10,15,20,255,0,31,255,0,255,31,0,255,224,0,153,255,0,0,0,255,255,71,0,0,235,255,0,173,255,31,0,255,11,200,200,255,82,0,0,255,245,0,61,255,0,255,112,0,255,133,255,0,0,255,163,0,255,102,0,194,255,0,0,143,255,51,255,0,0,82,255,0,255,41,0,255,173,10,0,255,173,255,0,0,255,153,255,92,0,255,0,255,255,0,245,255,0,102,255,173,0,255,0,20,255,184,184,0,31,255,0,255,61,0,71,255,255,0,204,0,255,194,0,255,82,0,10,255,0,112,255,51,0,255,0,194,255,0,122,255,0,255,163,255,153,0,0,255,10,255,112,0,143,255,0,82,0,255,163,255,0,255,235,0,8,184,170,133,0,255,0,255,92,184,0,255,255,0,31,0,184,255,0,214,255,255,0,112,92,255,0,0,224,255,112,224,255,70,184,160,163,0,255,153,0,255,71,255,0,255,0,163,255,204,0,255,0,143,0,255,235,133,255,0,255,0,235,245,0,255,255,0,122,255,245,0,10,190,212,214,255,0,0,204,255,20,0,255,255,255,0,0,153,255,0,41,255,0,255,204,41,0,255,41,255,0,173,0,255,0,245,255,71,0,255,122,0,255,0,255,184,0,92,255,184,255,0,0,133,255,255,214,0,25,194,194,102,255,0,92,0,255]\n",
    "\n",
    "\n",
    "\n",
    "def load(checkpoint_path, config, device):\n",
    "    pass\n",
    "    \n",
    "def get_legend_patch(npimg, new_palette, labels):\n",
    "    out_img = Image.fromarray(npimg.squeeze().astype('uint8'))\n",
    "    out_img.putpalette(new_palette)\n",
    "    u_index = np.unique(npimg)\n",
    "    patches = []\n",
    "    for i, index in enumerate(u_index):\n",
    "        label = labels[index]\n",
    "        cur_color = [new_palette[index * 3] / 255.0, new_palette[index * 3 + 1] / 255.0, new_palette[index * 3 + 2] / 255.0]\n",
    "        red_patch = mpatches.Patch(color=cur_color, label=label)\n",
    "        patches.append(red_patch)\n",
    "    return out_img, patches\n",
    "\n",
    "class LSeg_FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(LSeg_FeatureExtractor, self).__init__()\n",
    "        args = LSeg_args(weights='/home/MonoGS_Semantic/checkpoints/demo_e200.ckpt', \n",
    "                        data_path=None, \n",
    "                        dataset='ignore', \n",
    "                        backbone='clip_vitl16_384',\n",
    "                        aux=False,\n",
    "                        ignore_index = 255,\n",
    "                        scale_inv=False,\n",
    "                        widehead=True,\n",
    "                        widehead_hr=False,\n",
    "                        img_size=[480, 360])\n",
    "        \n",
    "        module = LSegModule.load_from_checkpoint(\n",
    "            checkpoint_path=args.weights,\n",
    "            data_path=args.data_path,\n",
    "            dataset=args.dataset,\n",
    "            backbone=args.backbone,\n",
    "            aux=args.aux,\n",
    "            num_features=256,\n",
    "            aux_weight=0,\n",
    "            se_loss=False,\n",
    "            se_weight=0,\n",
    "            base_lr=0,\n",
    "            batch_size=1,\n",
    "            max_epochs=0,\n",
    "            ignore_index=args.ignore_index,\n",
    "            dropout=0.0,\n",
    "            scale_inv=args.scale_inv,\n",
    "            augment=False,\n",
    "            no_batchnorm=False,\n",
    "            widehead=args.widehead,\n",
    "            widehead_hr=args.widehead_hr,\n",
    "            map_locatin=\"cpu\",\n",
    "            arch_option=0,\n",
    "            block_depth=0,\n",
    "            activation='lrelu',\n",
    "        )\n",
    "        self.labels = module.get_labels('ade20k', \"/home/MonoGS_Semantic/feature_encoder/lseg_encoder/\")\n",
    "        self.input_transform = module.val_transform\n",
    "        self.num_classes = len(self.labels)\n",
    "        \n",
    "        if isinstance(module.net, BaseNet):\n",
    "            model = module.net\n",
    "        else:\n",
    "            model = module\n",
    "            \n",
    "        model = model.eval()\n",
    "        model = model.cpu()\n",
    "        print(model)\n",
    "        \n",
    "        self.scales = [0.75, 1.0, 1.25, 1.75]\n",
    "        self.img_size = args.img_size\n",
    "        print(\"scales: \", self.scales)\n",
    "        print(\"img_size: \", self.img_size)\n",
    "        \n",
    "        self.evaluator = LSeg_MultiEvalModule(model, self.num_classes, scales=self.scales, flip=True).cuda()\n",
    "        self.evaluator.eval()\n",
    "        \n",
    "        self.debug = debug\n",
    "    \n",
    "    def _log(self, text):\n",
    "        if self.debug:\n",
    "            print(text)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def preprocess(self, image):\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "            image = self.input_transform(image).unsqueeze(0)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.input_transform(image).unsqueeze(0)\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            pass \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Supported types: str (file path), numpy.ndarray, torch.Tensor\")\n",
    "        self._log(f\"input size: {image.shape}\\n\")\n",
    "        image_tensor = F.interpolate(image, size=(self.img_size[1], self.img_size[0]),\n",
    "                                      mode=\"bilinear\", align_corners=True)\n",
    "        self._log(f\"resize size: {image_tensor.shape}\\n\")\n",
    "        return image_tensor\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward_feature(self, image: torch.Tensor):\n",
    "        output_features = self.evaluator.parallel_forward(image, return_feature=True)\n",
    "        return output_features[0].cpu().numpy().astype(np.float16)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, image):\n",
    "        image_tensor = self.preprocess(image)\n",
    "        return self.forward_feature(image_tensor)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def vis_feature(self, image, outname='test', outdir='vis'):\n",
    "        image_tensor = self.preprocess(image)\n",
    "        outputs = self.evaluator.parallel_forward(image_tensor)[0]\n",
    "        predicts = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "        \n",
    "        # save mask\n",
    "        masks = utils.get_mask_pallete(predicts, 'detail')\n",
    "        masks.save(os.path.join(outdir, outname+'.png'))\n",
    "        \n",
    "        # save vis\n",
    "        masks_tensor = torch.tensor(np.array(masks.convert(\"RGB\"), \"f\")) / 255.0\n",
    "        vis_img = (image_tensor[0] + 1) / 2.\n",
    "        vis_img = vis_img.permute(1, 2, 0)  # ->hwc\n",
    "        vis1 = vis_img\n",
    "        vis2 = vis_img * 0.4 + masks_tensor * 0.6\n",
    "        vis3 = masks_tensor\n",
    "        vis = torch.cat([vis1, vis2, vis3], dim=1)\n",
    "        Image.fromarray((vis.cpu().numpy() * 255).astype(np.uint8)).save(os.path.join(outdir, outname+\"_vis.png\"))\n",
    "\n",
    "        # save label vis\n",
    "        seg, patches = get_legend_patch(predicts, adepallete, self.labels)\n",
    "        seg = seg.convert(\"RGBA\")\n",
    "        plt.figure()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(seg)\n",
    "        plt.legend(handles=patches, prop={'size': 8}, ncol=4)\n",
    "        plt.savefig(os.path.join(outdir, outname+\"_legend.png\"), format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **\n",
      "LSegModule(\n",
      "  (net): LSegNet(\n",
      "    (clip_pretrained): CLIP(\n",
      "      (visual): VisionTransformer(\n",
      "        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer): Transformer(\n",
      "          (resblocks): Sequential(\n",
      "            (0): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): Sequential(\n",
      "          (0): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (11): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pretrained): Module(\n",
      "      (model): VisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "        (patch_drop): Identity()\n",
      "        (norm_pre): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (2): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (3): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (4): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (5): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (6): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (7): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (8): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (9): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (10): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (11): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (12): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (13): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (14): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (15): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (16): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (17): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (18): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (19): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (20): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (21): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (22): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "          (23): Block(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): Attention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): Identity()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): Identity()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (fc_norm): Identity()\n",
      "        (head_drop): Dropout(p=0.0, inplace=False)\n",
      "        (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "      )\n",
      "      (act_postprocess1): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (act_postprocess2): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (act_postprocess3): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (act_postprocess4): Sequential(\n",
      "        (0): ProjectReadout(\n",
      "          (project): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): Transpose()\n",
      "        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (scratch): Module(\n",
      "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (refinenet1): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet2): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet3): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet4): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output_conv): Sequential(\n",
      "        (0): Interpolate()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "scales:  [0.75, 1.0, 1.25, 1.75]\n",
      "img_size:  [480, 360]\n",
      "MultiEvalModule: base_size 520, crop_size 480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lseg = LSeg_FeatureExtractor(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([1, 3, 680, 1200])\n",
      "\n",
      "resize size: torch.Size([1, 3, 360, 480])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6314, 0.6314, 0.6158,  ..., 0.2941, 0.2941, 0.2863],\n",
       "          [0.6252, 0.6384, 0.6297,  ..., 0.2941, 0.2937, 0.2854],\n",
       "          [0.6235, 0.6401, 0.6209,  ..., 0.3002, 0.2941, 0.2924],\n",
       "          ...,\n",
       "          [0.4196, 0.4118, 0.4023,  ..., 0.7534, 0.7516, 0.7412],\n",
       "          [0.4118, 0.4149, 0.4039,  ..., 0.7308, 0.7591, 0.7482],\n",
       "          [0.4118, 0.4039, 0.3961,  ..., 0.6865, 0.7253, 0.7333]],\n",
       "\n",
       "         [[0.3490, 0.3490, 0.3568,  ..., 0.2078, 0.2078, 0.2000],\n",
       "          [0.3639, 0.3560, 0.3499,  ..., 0.2078, 0.2074, 0.1991],\n",
       "          [0.3647, 0.3577, 0.3508,  ..., 0.2017, 0.2078, 0.2061],\n",
       "          ...,\n",
       "          [0.2392, 0.2314, 0.2219,  ..., 0.3944, 0.4065, 0.3882],\n",
       "          [0.2314, 0.2345, 0.2235,  ..., 0.3788, 0.4140, 0.3952],\n",
       "          [0.2314, 0.2235, 0.2157,  ..., 0.3414, 0.3802, 0.3804]],\n",
       "\n",
       "         [[0.2549, 0.2549, 0.2393,  ..., 0.2550, 0.2549, 0.2471],\n",
       "          [0.2488, 0.2479, 0.2393,  ..., 0.2549, 0.2545, 0.2602],\n",
       "          [0.2471, 0.2479, 0.2533,  ..., 0.2549, 0.2487, 0.2689],\n",
       "          ...,\n",
       "          [0.1260, 0.1093, 0.0965,  ..., 0.1068, 0.1006, 0.0824],\n",
       "          [0.1059, 0.1247, 0.0980,  ..., 0.0746, 0.1081, 0.0876],\n",
       "          [0.1059, 0.1137, 0.0903,  ..., 0.0511, 0.0744, 0.0588]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_path = \"/home/data/datasets/replica/room0/results/frame000001.jpg\"\n",
    "lseg.preprocess(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([1, 3, 680, 1200])\n",
      "\n",
      "resize size: torch.Size([1, 3, 360, 480])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/feature_3dgs/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    }
   ],
   "source": [
    "lseg.vis_feature(test_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonoGS_Semantic_feat_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
